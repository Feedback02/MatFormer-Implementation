{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/host/.local/lib/python3.8/site-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/home/host/.local/lib/python3.8/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/home/host/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# For some reason needed or torchtext will not work...\n",
        "torch.utils.data.datapipes.utils.common.DILL_AVAILABLE = torch.utils._import_utils.dill_available()\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torchtext.datasets import IMDB\n",
        "import re\n",
        "from transformers import AutoTokenizer,AutoModel\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.datasets import IMDB\n",
        "import torch.nn.functional as F\n",
        "\n",
        "train_iter, test_iter = IMDB()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure the reproducibility of results\n",
        "\n",
        "from transformers import set_seed\n",
        "\n",
        "seed = 42\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Set Seed for transfomers\n",
        "set_seed(seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def tokenize_and_encode(batch, tokenizer, max_length=512):\n",
        "    inputs = tokenizer.batch_encode_plus(\n",
        "        batch,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    return inputs['input_ids'], inputs['attention_mask'].to(torch.bool)\n",
        "\n",
        "\n",
        "def process_dataset(iterator, tokenizer):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    #label 1 is negative, 2 is positive\n",
        "    for i, (label, text) in enumerate(iterator):\n",
        "        cleaned_text = clean_text(text)\n",
        "        texts.append(cleaned_text)\n",
        "        labels.append(1 if label == 2 else 0)\n",
        "\n",
        "\n",
        "    input_ids, attention_masks = tokenize_and_encode(texts, tokenizer)\n",
        "    labels = torch.tensor(labels)\n",
        "    return input_ids, attention_masks, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/host/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('prajjwal1/bert-mini')#'kanishka/GlossBERT')\n",
        "\n",
        "train_input_ids, train_attention_masks, train_labels = process_dataset(train_iter, tokenizer)\n",
        "test_input_ids, test_attention_masks, test_labels = process_dataset(test_iter, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "vocab_size = tokenizer.vocab_size\n",
        "\n",
        "train_data = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "test_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
        "test_sampler = SequentialSampler(test_data) # SequentialSampler \n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 512, 768])\n",
            "torch.Size([32, 768])\n"
          ]
        }
      ],
      "source": [
        "# Used only for testing\n",
        "\n",
        "#for elem in train_dataloader:\n",
        "#    input_ids_batch, attention_mask_batch, target_batch = elem\n",
        "#    output = model(input_ids_batch.to(device), attention_mask = attention_mask_batch.to(device), granularity_level=3)\n",
        "#    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KqmtGGA_tNjq"
      },
      "outputs": [],
      "source": [
        "class NestedFFN(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, num_granularities=4):\n",
        "        super(NestedFFN, self).__init__()\n",
        "\n",
        "        # Initialize FFN layers\n",
        "        self.num_granularities = num_granularities\n",
        "        self.d_model = d_model\n",
        "        self.d_ff = d_ff\n",
        "\n",
        "        # Create weight matrices for W1 and W2 with the largest size\n",
        "        self.W1 = nn.Parameter(torch.randn(d_ff, d_model))\n",
        "        self.W2 = nn.Parameter(torch.randn(d_ff, d_model))\n",
        "\n",
        "        # Calculate the sizes of each granularity\n",
        "        self.granularity_sizes = [d_ff // (2 ** i) for i in range(num_granularities)]\n",
        "        self.granularity_sizes_mix = []\n",
        "\n",
        "        # This is for mix' n' match\n",
        "        for i in range(num_granularities-1) :\n",
        "            self.granularity_sizes_mix.append(int(1/2 * (self.granularity_sizes[i] + self.granularity_sizes[i+1])))\n",
        "\n",
        "        #print(self.granularity_sizes_mix)\n",
        "\n",
        "    def forward(self, x, granularity_level):\n",
        "        assert 0 <= granularity_level < self.num_granularities, \"Invalid granularity level\"\n",
        "\n",
        "        # m_i Number of neuron selected\n",
        "        m_i = self.granularity_sizes[granularity_level]\n",
        "\n",
        "        # Perform the FFN operation with the selected subset of weights\n",
        "        hidden = F.gelu(x @ self.W1[:m_i, :].T)\n",
        "        output = F.gelu( hidden @ self.W2[:m_i, :] )\n",
        "\n",
        "        return output\n",
        "    \n",
        "    # This function is used only at inference, where we choose different granulaties that the model is not explicitly trained on that granularities\n",
        "    def forward_mix(self, x, granularity_level):\n",
        "        # m_i Number of neuron selected\n",
        "        m_i = self.granularity_sizes_mix[granularity_level]\n",
        "\n",
        "        # Perform the FFN operation with the selected subset of weights\n",
        "        hidden = F.gelu(x @ self.W1[:m_i, :].T)\n",
        "        output = F.gelu( hidden @ self.W2[:m_i, :] )\n",
        "\n",
        "        return output\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, nested_ffn, granularity_level, dropout=0.1):\n",
        "        super(TransformerLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
        "        self.granularity_level = granularity_level\n",
        "        self.nested_ffn = nested_ffn\n",
        "        self.layernorm1 = nn.LayerNorm(d_model)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, inference, src_mask=None, src_key_padding_mask=None):\n",
        "        src2, _ = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)\n",
        "        src = src + self.dropout(src2)\n",
        "        src = self.layernorm1(src)\n",
        "\n",
        "        if inference == False:\n",
        "            src2 = self.nested_ffn(src, self.granularity_level)\n",
        "        else:\n",
        "            src2 = self.nested_ffn.forward_mix(src, self.granularity_level)\n",
        "\n",
        "        src = src + self.dropout(src2)\n",
        "        src = self.layernorm2(src)\n",
        "        return src\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, d_model, num_layers, num_heads, nested_ffn, num_granularities=4, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.models = [ ]\n",
        "        # We Stack l Layers with the same granularity_level\n",
        "        # Creating M1, M2, ... , Mg\n",
        "        for id in range(num_granularities):\n",
        "\n",
        "          self.models.append( nn.ModuleList([\n",
        "            TransformerLayer(d_model, num_heads, nested_ffn, id, dropout).to(device)\n",
        "            for _ in range(num_layers)\n",
        "          ]))\n",
        "\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None, granularity_level = 0, inference = False):\n",
        "      # So granularity_level indicates the model M_i that we want to use\n",
        "        for layer in self.models[granularity_level]:\n",
        "            src = layer(src, inference, src_mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
        "        src = self.layernorm(src)\n",
        "        return src\n",
        "    \n",
        "\n",
        "class SentimentTransformer(nn.Module):\n",
        "    def __init__(self,d_ff, num_layers, num_heads, granularity_levels=4, dropout=0.1,num_granularities = 4):\n",
        "        super(SentimentTransformer, self).__init__()\n",
        "\n",
        "        self.bert = AutoModel.from_pretrained('prajjwal1/bert-mini') #'kanishka/GlossBERT') #, torch_dtype=torch.float16)\n",
        "\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        d_model = self.bert.config.hidden_size\n",
        "        self.nested_ffn = NestedFFN(d_model, d_ff, num_granularities)\n",
        "\n",
        "        self.transformer = Transformer(d_model, num_layers, num_heads, self.nested_ffn, granularity_levels, dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(d_model, 512)\n",
        "        self.fc2 = nn.Linear(512, 1)  # Binary classification\n",
        "        \n",
        "\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask=None, granularity_level=0, inference = False):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        hidden_states = outputs.last_hidden_state \n",
        "\n",
        "        src = self.transformer(hidden_states, granularity_level=granularity_level, inference = inference)\n",
        "        src = self.relu(self.fc1(torch.mean(src, dim=1)))\n",
        "        \n",
        "        src = self.fc2(src)\n",
        "\n",
        "        return src"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "d_ff = 2048\n",
        "num_granularities = 4\n",
        "num_layers = 2  \n",
        "num_heads = 4 \n",
        "dropout = 0\n",
        "epochs = 10 \n",
        "learning_rate = 0.001\n",
        "\n",
        "# Hyperparameters for single transformer, the rest is the same\n",
        "#num_granularities = 1\n",
        "#num_layers = 1  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model = SentimentTransformer( d_ff, num_layers, num_heads, dropout=dropout,num_granularities=num_granularities).to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total parameters: 12351745\n",
            "Trainable parameters: 1181185\n"
          ]
        }
      ],
      "source": [
        "# Count the total number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "# Count the number of parameters that require gradients\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total parameters: {total_params}\")\n",
        "print(f\"Trainable parameters: {trainable_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TY7PeeZ14Bw",
        "outputId": "e66e053e-e4a8-495b-eace-acdb8b387835"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [11:55<00:00,  1.09batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 12.342774744145572\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [19:05<00:00,  1.46s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10, Loss: 11.283162438310683\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [19:56<00:00,  1.53s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10, Loss: 10.998607359360904\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [20:15<00:00,  1.55s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10, Loss: 10.67746351333335\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [20:23<00:00,  1.57s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10, Loss: 10.388783525675535\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [20:08<00:00,  1.55s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10, Loss: 9.99887270713225\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [19:14<00:00,  1.48s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10, Loss: 9.659495792817324\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [19:12<00:00,  1.47s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10, Loss: 9.365285453386605\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [18:48<00:00,  1.44s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10, Loss: 9.054775128606707\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [19:12<00:00,  1.47s/batch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10, Loss: 8.775679647922516\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model.train()\n",
        "\n",
        "# Initialize the model\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    for batch in tqdm(train_dataloader, unit='batch'):\n",
        "        input_ids_batch, attention_mask_batch, target_batch = batch\n",
        "\n",
        "        input_ids_batch = input_ids_batch.to(device)\n",
        "        attention_mask_batch = attention_mask_batch.to(device)\n",
        "        target_batch = target_batch.to(device)\n",
        "        \n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Compute the loss for each granularity level and combine them\n",
        "        losses = []\n",
        "        for granularity_level in range(num_granularities):\n",
        "\n",
        "            output = model(input_ids_batch, attention_mask = attention_mask_batch, granularity_level=granularity_level)\n",
        "            loss = criterion(output.flatten(), target_batch.float())\n",
        "            losses.append(loss)\n",
        "            \n",
        "        # Combine the losses\n",
        "        combined_loss = sum(losses) / num_granularities\n",
        "\n",
        "        # Backpropagation\n",
        "        combined_loss.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss for reporting\n",
        "        total_loss += combined_loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/batch_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [16:23<00:00,  1.26s/batch]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model.eval()\n",
        "\n",
        "# Number of correct prediction made by M1,...Mg\n",
        "correct_model = [ 0 for _ in range(num_granularities) ]\n",
        "# Number of correct prediction made by mix' n' match\n",
        "correct_model_mix = [ 0 for _ in range(len(model.nested_ffn.granularity_sizes_mix)) ]\n",
        "\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_dataloader, unit='batch'):\n",
        "        input_ids_batch, attention_mask_batch, target_batch = batch\n",
        "        \n",
        "        input_ids_batch = input_ids_batch.to(device)\n",
        "        attention_mask_batch = attention_mask_batch.to(device)\n",
        "        target_batch = target_batch.to(device)\n",
        "\n",
        "        for granularity_level in range(num_granularities):\n",
        "            output = model(input_ids_batch, attention_mask = attention_mask_batch, granularity_level=granularity_level)\n",
        "            output = torch.sigmoid(output)\n",
        "            output = torch.tensor([True if prob >0.5 else False for prob in output.flatten()]).to(device)\n",
        "\n",
        "            correct_prediction = torch.eq(output, target_batch)\n",
        "\n",
        "            correct_model[granularity_level] += torch.sum(correct_prediction).item()\n",
        "\n",
        "        for granularity_level in range(len(model.nested_ffn.granularity_sizes_mix)):\n",
        "            output = model(input_ids_batch, attention_mask = attention_mask_batch, granularity_level=granularity_level, inference = True)\n",
        "            output = torch.sigmoid(output)\n",
        "            output = torch.tensor([True if prob >0.5 else False for prob in output.flatten()]).to(device)\n",
        "\n",
        "            correct_prediction = torch.eq(output, target_batch)\n",
        "\n",
        "            correct_model_mix[granularity_level] += torch.sum(correct_prediction).item()\n",
        "\n",
        "        total += torch.sum(target_batch)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of each sub-model [0.82616, 0.82548, 0.80956, 0.79756]\n",
            "Each sub-model has number of neurons: [2048, 1024, 512, 256]\n",
            "So for example, the first sub model with  2048 neurons, has an accuracy of  0.82616\n"
          ]
        }
      ],
      "source": [
        "accuracy = [n_correct/len(test_data) for n_correct in correct_model]\n",
        "\n",
        "n_sub_models_neurons = [d_ff // (2 ** i) for i in range(num_granularities)]\n",
        "\n",
        "print('Accuracy of each sub-model', accuracy)\n",
        "print('Each sub-model has number of neurons:', n_sub_models_neurons )\n",
        "print(\"So for example, the first sub model with \", n_sub_models_neurons[0], \"neurons, has an accuracy of \", accuracy[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of each sub-model with different granularities [0.81008, 0.77784, 0.78828]\n",
            "Each sub-model has number of neurons: [1536, 768, 384]\n",
            "So for example, the first sub model with  1536 neurons, has an accuracy of  0.81008\n"
          ]
        }
      ],
      "source": [
        "accuracy = [n_correct/len(test_data) for n_correct in correct_model_mix]\n",
        "\n",
        "n_sub_models_neurons_mix = model.nested_ffn.granularity_sizes_mix\n",
        "\n",
        "print('Accuracy of each sub-model with different granularities', accuracy)\n",
        "print('Each sub-model has number of neurons:', n_sub_models_neurons_mix )\n",
        "print(\"So for example, the first sub model with \", n_sub_models_neurons_mix[0], \"neurons, has an accuracy of \", accuracy[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(),'model_weights_transformer_2')\n",
        "#model.load_state_dict(torch.load('model_weights_1'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
