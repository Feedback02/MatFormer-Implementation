{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# MatFormer: Nested Transformer for Elastic Inference\n",
        "\n",
        "## Introduction\n",
        "This notebook provides the implementation of the MatFormer model as described in the paper \"MatFormer: Nested Transformer for Elastic Inference\". The MatFormer introduces a nested Transformer architecture designed to offer elasticity in a variety of deployment constraints.\n",
        "\n",
        "## Description of the Method\n",
        "The MatFormer architecture is based on the concept of nested sub-structures within the Transformer model. Each Feed Forward Network (FFN) block of a MatFormer model is jointly optimized with a few nested smaller FFN blocks, enabling the extraction of smaller sub-models from a larger, universal model.\n",
        "To evaluate the model, we are going to use IMDB dataset.\n",
        "\n",
        "### Key Components of the architecture\n",
        "1. **Nested FFN Blocks:** The FFN block in the Transformer is modified to include a nested structure, allowing for multiple granularities within a single model.\n",
        "2. **Mix’n’Match:** This approach allows for the combination of different granularities across layers, generating numerous sub-models without additional training.\n",
        "## Implementation Details\n",
        "\n",
        "### Loading Libraries and Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/host/.local/lib/python3.8/site-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/home/host/.local/lib/python3.8/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/home/host/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# For some reason needed or torchtext will not work...\n",
        "torch.utils.data.datapipes.utils.common.DILL_AVAILABLE = torch.utils._import_utils.dill_available()\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torchtext.datasets import IMDB\n",
        "import re\n",
        "from transformers import AutoTokenizer,AutoModel\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.datasets import IMDB\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "train_iter, test_iter = IMDB(split=('train','test'))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure the reproducibility of results\n",
        "from transformers import set_seed\n",
        "\n",
        "seed = 42\n",
        "\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Set Seed for transfomers\n",
        "set_seed(seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Processing\n",
        "\n",
        "To train and evaluate the MatFormer model, we need to preprocess the dataset. This involves cleaning the text data, tokenizing and encoding it into a format suitable for the model, and organizing it into batches for training and evaluation. Below we have functions used for these preprocessing steps.\n",
        "1. **clean_text:** Responsible for lowercasing the text, removing HTML tags and unwanted characters, and ensuring that the data is in a consistent format. This step helps in reducing noise and improving the quality of the input data.\n",
        "\n",
        "2. **tokenize_and_encode:** Takes the cleaned text and converts it into a sequence of tokens, then encodes these tokens into numerical values that can be processed by the model.\n",
        "\n",
        "3. **process_dataset:** Applies the cleaning, tokenizing, and encoding steps to the entire dataset. It then organizes the data into batches for training and evaluation. This function ensures that the data is ready to be fed into the MatFormer model.\n",
        "\n",
        "Then as tokenizer we are going to use bert, the model matFormer in evaluation will exploit the bert pretrained model freezing its parameters in training phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def tokenize_and_encode(batch, tokenizer, max_length=512):\n",
        "    inputs = tokenizer.batch_encode_plus(\n",
        "        batch,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    return inputs['input_ids'], inputs['attention_mask'].to(torch.bool)\n",
        "\n",
        "\n",
        "def process_dataset(iterator, tokenizer):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    #label 1 is negative, 2 is positive\n",
        "    for i, (label, text) in enumerate(iterator):\n",
        "        cleaned_text = clean_text(text)\n",
        "        texts.append(cleaned_text)\n",
        "        labels.append(1 if label == 2 else 0)\n",
        "\n",
        "\n",
        "    input_ids, attention_masks = tokenize_and_encode(texts, tokenizer)\n",
        "    labels = torch.tensor(labels)\n",
        "    return input_ids, attention_masks, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/host/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('prajjwal1/bert-mini')#'kanishka/GlossBERT')\n",
        "\n",
        "train_input_ids, train_attention_masks, train_labels = process_dataset(train_iter, tokenizer)\n",
        "test_input_ids, test_attention_masks, test_labels = process_dataset(test_iter, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "vocab_size = tokenizer.vocab_size\n",
        "\n",
        "train_data = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "test_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
        "test_sampler = SequentialSampler(test_data) \n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 512, 768])\n",
            "torch.Size([32, 768])\n"
          ]
        }
      ],
      "source": [
        "# Used only for testing\n",
        "\n",
        "#for elem in train_dataloader:\n",
        "#    input_ids_batch, attention_mask_batch, target_batch = elem\n",
        "#    output = model(input_ids_batch.to(device), attention_mask = attention_mask_batch.to(device), granularity_level=3)\n",
        "#    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model definition\n",
        "### MatFormer Structure\n",
        "\n",
        "The MatFormer model defines $( g )$ Transformer blocks $( T_i )$ such that $( T_1 \\subset T_2 \\subset \\cdots \\subset T_g )$, where $( T_i \\subset T_{i+1} )$ indicates that the parameters of $( T_i )$ are contained in those of $( T_{i+1} )$.\n",
        "\n",
        "While it is possible to impose such a structure on any part of the Transformer, we select the Feed Forward Network (FFN) block to define our method and present our experiments. The model size and computational cost of a Transformer are dominated (around 60% for LLMs and ViTs) by the FFN block.\n",
        "\n",
        "The FFN block in a Transformer has a single hidden layer with $( d_{ff} )$ neurons and both input and outputs in $( \\mathbb{R}^{d_{model}} )$, and a fixed FFN ratio $( := \\frac{d_{ff}}{d_{model}} )$ (typically $( \\geq 4 )$). MatFormer introduces the matryoshka nested structure with $( g )$ granularities on the hidden representation $( d_{ff} )$ of the FFN block.\n",
        "\n",
        "Concretely, a nested sub-block of the Transformer, $( T_i )$, contains the first $( m_i )$ neurons of the FFN, and $( 1 \\leq m_1 \\leq m_2 \\cdots \\leq m_g = d_{ff} )$ represent the number of neurons for each granularity or sub-model.\n",
        "\n",
        "So, depending on the chosen granularity, the FFN operation of $( T_i )$, i.e., $( T_{FFN}^i )$ on an input $( x \\in \\mathbb{R}^{d_{model}} )$, is:\n",
        "\n",
        "\n",
        "$\n",
        "T_{FFN}^i(x) = \\sigma(x \\cdot W_1[0 : m_i]^T) \\cdot W_2[0 : m_i],\n",
        "$\n",
        "\n",
        "where the weight matrices of FFN are $( W_1, W_2 \\in \\mathbb{R}^{d_{ff} \\times d_{model}} )$ and bias terms are omitted for simplicity. $( W_1[0 : k] )$ denotes the submatrix with the first $( k )$ rows of $( W_1 )$. Finally, $( \\sigma )$ is a non-linearity often set to GELU (Gaussian Error Linear Unit) or squared ReLU.\n",
        "\n",
        "In this work, we chose the $( g = 4 )$ exponentially spaced granularities with FFN ratios of $( \\{0.5, 1, 2, 4\\} )$, i.e., the nested hidden neurons are of the sizes $( \\left\\{\\frac{d_{ff}}{8}, \\frac{d_{ff}}{4}, \\frac{d_{ff}}{2}, d_{ff}\\right\\} )$.\n",
        "\n",
        "With the nested MatFormer blocks $( T_1, T_2, \\ldots, T_g )$, we can combine these to form a MatFormer model, with $( g )$ nested submodels $( M_1 \\subset M_2 \\cdots \\subset M_g )$, where $( M_i \\leftarrow [T_i]^l )$, i.e., $( M_i )$ is formed by stacking $( T_i )$ for $( l )$ layers. The input and output embedding matrices are shared across the models.\n",
        "\n",
        "Below the implementation.\n",
        "### Mix’n’Match\n",
        "\n",
        "The Mix’n’Match strategy in the MatFormer model allows for the extraction of a combinatorially large number of accurate and smaller submodels from a single trained model. This is achieved by selecting different granularities for each MatFormer layer during inference, enabling the generation of models tailored to specific computational constraints without additional training.\n",
        "\n",
        "**Key Points:**\n",
        "1. **Dynamic Model Extraction**: Mix’n’Match enables the dynamic extraction of smaller models by selecting different subsets of neurons at each layer.\n",
        "2. **Combinatorial Flexibility**: By choosing different granularities across layers, it is possible to create a large variety of submodels that meet specific accuracy and computational trade-offs.\n",
        "3. **Interpolation**: Interpolating between granularities can also produce highly accurate models.\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "An interpolated block $ \\tilde{T} $ that uses a mix of neurons from two consecutive granularities $ m_i $ and $ m_{i+1}$ is defined as:\n",
        "\n",
        "$\n",
        "\\tilde{T} = \\frac{1}{2} (m_i + m_{i+1})\n",
        "$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KqmtGGA_tNjq"
      },
      "outputs": [],
      "source": [
        "class NestedFFN(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, num_granularities=4):\n",
        "        super(NestedFFN, self).__init__()\n",
        "\n",
        "        # Initialize FFN layers\n",
        "        self.num_granularities = num_granularities\n",
        "        self.d_model = d_model\n",
        "        self.d_ff = d_ff\n",
        "\n",
        "        # Create weight matrices for W1 and W2 with the largest size\n",
        "        self.W1 = nn.Parameter(torch.randn(d_ff, d_model))\n",
        "        self.W2 = nn.Parameter(torch.randn(d_ff, d_model))\n",
        "\n",
        "        # Create bias vectors for W1 and W2 with the largest size\n",
        "        self.b1 = nn.Parameter(torch.randn(d_ff))\n",
        "        self.b2 = nn.Parameter(torch.randn(d_model))\n",
        "\n",
        "        # Calculate the sizes of each granularity\n",
        "        self.granularity_sizes = [d_ff // (2 ** i) for i in range(num_granularities)]\n",
        "        self.granularity_sizes_mix = []\n",
        "\n",
        "        # This is for mix' n' match\n",
        "        for i in range(num_granularities-1) :\n",
        "            self.granularity_sizes_mix.append(int(1/2 * (self.granularity_sizes[i] + self.granularity_sizes[i+1])))\n",
        "\n",
        "        #print(self.granularity_sizes_mix)\n",
        "\n",
        "    def forward(self, x, granularity_level):\n",
        "        assert 0 <= granularity_level < self.num_granularities, \"Invalid granularity level\"\n",
        "\n",
        "        # m_i Number of neuron selected\n",
        "        m_i = self.granularity_sizes[granularity_level]\n",
        "\n",
        "        # Perform the FFN operation with the selected subset of weights\n",
        "        hidden = F.gelu(x @ self.W1[:m_i, :].T + self.b1[:m_i])\n",
        "        output = hidden @ self.W2[:m_i, :] + self.b2[:m_i]\n",
        "\n",
        "        return output\n",
        "    \n",
        "    # This function is used only at inference, where we choose different granulaties that the model is not explicitly trained on that granularities\n",
        "    def forward_mix(self, x, granularity_level):\n",
        "        # m_i Number of neuron selected\n",
        "        m_i = self.granularity_sizes_mix[granularity_level]\n",
        "\n",
        "        # Perform the FFN operation with the selected subset of weights\n",
        "        hidden = F.gelu(x @ self.W1[:m_i, :].T + self.b1[:m_i])\n",
        "        output = hidden @ self.W2[:m_i, :] + self.b2[:m_i]\n",
        "\n",
        "        return output\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, nested_ffn, granularity_level, dropout=0.1):\n",
        "        super(TransformerLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
        "        self.granularity_level = granularity_level\n",
        "        self.nested_ffn = nested_ffn\n",
        "        self.layernorm1 = nn.LayerNorm(d_model)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, inference, src_mask=None, src_key_padding_mask=None):\n",
        "        src2, _ = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)\n",
        "        src = src + self.dropout(src2)\n",
        "        src = self.layernorm1(src)\n",
        "\n",
        "        if inference == False:\n",
        "            src2 = self.nested_ffn(src, self.granularity_level)\n",
        "        else:\n",
        "            src2 = self.nested_ffn.forward_mix(src, self.granularity_level)\n",
        "\n",
        "        src = src + self.dropout(src2)\n",
        "        src = self.layernorm2(src)\n",
        "        return src\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, d_model, num_layers, num_heads, nested_ffn, num_granularities=4, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.models = [ ]\n",
        "        # We Stack l Layers with the same granularity_level\n",
        "        # Creating M1, M2, ... , Mg\n",
        "        for id in range(num_granularities):\n",
        "\n",
        "          self.models.append( nn.ModuleList([\n",
        "            TransformerLayer(d_model, num_heads, nested_ffn, id, dropout).to(device)\n",
        "            for _ in range(num_layers)\n",
        "          ]))\n",
        "\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None, granularity_level = 0, inference = False):\n",
        "      # So granularity_level indicates the model M_i that we want to use\n",
        "        for layer in self.models[granularity_level]:\n",
        "            src = layer(src, inference, src_mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
        "        src = self.layernorm(src)\n",
        "        return src\n",
        "    \n",
        "\n",
        "class SentimentTransformer(nn.Module):\n",
        "    def __init__(self,d_ff, num_layers, num_heads, granularity_levels=4, dropout=0.1,num_granularities = 4):\n",
        "        super(SentimentTransformer, self).__init__()\n",
        "\n",
        "        # Load pre trained BERT model\n",
        "        self.bert = AutoModel.from_pretrained('prajjwal1/bert-mini') #'kanishka/GlossBERT') #, torch_dtype=torch.float16)\n",
        "        \n",
        "        # Freeze BERT parameters\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.d_model = self.bert.config.hidden_size\n",
        "        self.nested_ffn = NestedFFN(self.d_model, d_ff, num_granularities)\n",
        "\n",
        "        self.transformer = Transformer(self.d_model, num_layers, num_heads, self.nested_ffn, granularity_levels, dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(self.d_model, 512)\n",
        "        self.fc2 = nn.Linear(512, 1)  # Binary classification\n",
        "        \n",
        "\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask=None, granularity_level=0, inference = False):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        hidden_states = outputs.last_hidden_state \n",
        "\n",
        "        src = self.transformer(hidden_states, granularity_level=granularity_level, inference = inference)\n",
        "        src = self.relu(self.fc1(torch.mean(src, dim=1)))\n",
        "        \n",
        "        src = self.fc2(src)\n",
        "\n",
        "        return src"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training\n",
        "\n",
        "The training strategy for the MatFormer model involves jointly optimizing all the nested submodels. This is done by defining a joint loss function that combines the loss of each submodel with specific weights. The training process ensures that each submodel is accurate and consistent with the others, allowing for efficient extraction of smaller models.\n",
        "\n",
        "**Key Points:**\n",
        "1. **Joint Optimization**: All the granular submodels are optimized together using a combined loss function.\n",
        "2. **Loss Function**: The joint loss function is a weighted average of the individual losses of each submodel.\n",
        "3. **Efficiency**: This training strategy is more efficient than training each submodel independently and ensures consistency across submodels.\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "The joint loss function $( L_{JOINT} )$ is defined as:\n",
        "\n",
        "$\n",
        "L_{JOINT}(x, y) = \\sum_{i=1}^{g} \\lambda_i \\cdot L(M_i(x), y)\n",
        "$\n",
        "\n",
        "where:\n",
        "- $( x )$ is the input.\n",
        "- $( y )$ is the target.\n",
        "- $( M_i )$ is the $( i )$-th granular submodel.\n",
        "- $( \\lambda_i )$ is the weight for the $( i )$-th submodel's loss.\n",
        "- $( L )$ is the loss function (e.g., cross-entropy loss).\n",
        "\n",
        "A possible choice can be $( \\lambda_i  = \\frac{1}{number granularities} )$ for all i\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "d_ff = 2048\n",
        "num_granularities = 4\n",
        "num_layers = 2  \n",
        "num_heads = 4 \n",
        "dropout = 0\n",
        "epochs = 10 \n",
        "learning_rate = 0.001\n",
        "\n",
        "# Hyperparameters for single transformer, the rest is the same\n",
        "#num_granularities = 1\n",
        "#num_layers = 1  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model = SentimentTransformer( d_ff, num_layers, num_heads, dropout=dropout,num_granularities=num_granularities).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total parameters: 12354049\n",
            "Trainable parameters: 1183489\n"
          ]
        }
      ],
      "source": [
        "# Count the total number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "# Count the number of parameters that require gradients\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total parameters: {total_params}\")\n",
        "print(f\"Trainable parameters: {trainable_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TY7PeeZ14Bw",
        "outputId": "e66e053e-e4a8-495b-eace-acdb8b387835"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 18%|█▊        | 140/782 [01:05<05:37,  1.90batch/s]"
          ]
        }
      ],
      "source": [
        "\n",
        "model.train()\n",
        "\n",
        "# Initialize the model\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    for batch in tqdm(train_dataloader, unit='batch'):\n",
        "        input_ids_batch, attention_mask_batch, target_batch = batch\n",
        "\n",
        "        input_ids_batch = input_ids_batch.to(device)\n",
        "        attention_mask_batch = attention_mask_batch.to(device)\n",
        "        target_batch = target_batch.to(device)\n",
        "        \n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Compute the loss for each granularity level and combine them\n",
        "        losses = []\n",
        "        for granularity_level in range(num_granularities):\n",
        "\n",
        "            output = model(input_ids_batch, attention_mask = attention_mask_batch, granularity_level=granularity_level)\n",
        "            loss = criterion(output.flatten(), target_batch.float())\n",
        "            losses.append(loss)\n",
        "            \n",
        "        # Combine the losses\n",
        "        combined_loss = sum(losses) / num_granularities\n",
        "\n",
        "        # Backpropagation\n",
        "        combined_loss.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss for reporting\n",
        "        total_loss += combined_loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/batch_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation\n",
        "\n",
        "We evaluate the models $( M_1, \\ldots, M_g $) ( in our case g = 4 ) as well as other models that can be constructed using different numbers of neurons. These additional models, which are not explicitly trained like $( M_1, \\ldots, M_g $), still demonstrate impressive performance.\n",
        "\n",
        "**Key Points:**\n",
        "1. **Evaluation of Explicitly Trained Models**: We assess the performance of the explicitly trained models $( M_1, \\ldots, M_g $).\n",
        "2. **Evaluation of Constructed Models**: We also evaluate models constructed using various subsets of neurons. These models leverage the nested structure of MatFormer, allowing them to perform well even without explicit training.\n",
        "3. **Performance**: The constructed models show strong performance, indicating the effectiveness of the nested training approach.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model.eval()\n",
        "\n",
        "# Number of correct prediction made by M1,...Mg\n",
        "correct_model = [ 0 for _ in range(num_granularities) ]\n",
        "# Number of correct prediction made by mix' n' match\n",
        "correct_model_mix = [ 0 for _ in range(len(model.nested_ffn.granularity_sizes_mix)) ]\n",
        "\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_dataloader, unit='batch'):\n",
        "        input_ids_batch, attention_mask_batch, target_batch = batch\n",
        "        \n",
        "        input_ids_batch = input_ids_batch.to(device)\n",
        "        attention_mask_batch = attention_mask_batch.to(device)\n",
        "        target_batch = target_batch.to(device)\n",
        "        # Evaluation of M1,...,Mg\n",
        "        for granularity_level in range(num_granularities):\n",
        "            output = model(input_ids_batch, attention_mask = attention_mask_batch, granularity_level=granularity_level)\n",
        "            output = torch.sigmoid(output)\n",
        "            output = torch.tensor([True if prob >0.5 else False for prob in output.flatten()]).to(device)\n",
        "\n",
        "            correct_prediction = torch.eq(output, target_batch)\n",
        "\n",
        "            correct_model[granularity_level] += torch.sum(correct_prediction).item()\n",
        "\n",
        "        # Evaluation of mix n' match models (so with different granularities)\n",
        "        for granularity_level in range(len(model.nested_ffn.granularity_sizes_mix)):\n",
        "            output = model(input_ids_batch, attention_mask = attention_mask_batch, granularity_level=granularity_level, inference = True)\n",
        "            output = torch.sigmoid(output)\n",
        "            output = torch.tensor([True if prob >0.5 else False for prob in output.flatten()]).to(device)\n",
        "\n",
        "            correct_prediction = torch.eq(output, target_batch)\n",
        "\n",
        "            correct_model_mix[granularity_level] += torch.sum(correct_prediction).item()\n",
        "\n",
        "        total += torch.sum(target_batch)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy = [n_correct/len(test_data) for n_correct in correct_model]\n",
        "\n",
        "n_sub_models_neurons = [d_ff // (2 ** i) for i in range(num_granularities)]\n",
        "\n",
        "print('Accuracy of each sub-model', accuracy)\n",
        "print('Each sub-model has number of neurons:', n_sub_models_neurons )\n",
        "print(\"So for example, the first sub model with \", n_sub_models_neurons[0], \"neurons, has an accuracy of \", accuracy[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy = [n_correct/len(test_data) for n_correct in correct_model_mix]\n",
        "\n",
        "n_sub_models_neurons_mix = model.nested_ffn.granularity_sizes_mix\n",
        "\n",
        "print('Accuracy of each sub-model with different granularities', accuracy)\n",
        "print('Each sub-model has number of neurons:', n_sub_models_neurons_mix )\n",
        "print(\"So for example, the first sub model with \", n_sub_models_neurons_mix[0], \"neurons, has an accuracy of \", accuracy[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#torch.save(model.state_dict(),'1_model_weights_traditional')\n",
        "#model.eval()\n",
        "model.load_state_dict(torch.load('1_model_weights_traditional'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results\n",
        "\n",
        "Below, we have the difference of loss in training stage between matFormer and the traditional transformer with traditional training\n",
        "<br>\n",
        "\n",
        "![Alt Test](loss_epochs.png)\n",
        "\n",
        "<br>\n",
        "Below, we have matFormer accuracy of the sub-models M1,M2,M3,M4 using IMDB dataset sentiment analysis\n",
        "<br>\n",
        "\n",
        "![Alt Test](accuracy_1.png)\n",
        "\n",
        "<br>\n",
        "Below, we have matFormer accuracy of the sub-models using Mix' n' Match tecnique, using IMDB dataset sentiment analysis\n",
        "<br>\n",
        "\n",
        "![Alt Test](accuracy_2.png)\n",
        "\n",
        "\n",
        "<br>\n",
        "For the accuracy of the traditional transformer, we got 0.81556 (with 2048 neurons)\n",
        "<br>\n",
        "\n",
        "## Conclusions\n",
        "\n",
        "The evaluation of the sub-models $M_1, M_2, M_3, M_4 $ reveals that their accuracies are relatively similar. It suggests that we can opt for the smaller sub-model $ M_4 $ without experiencing a substantial loss in accuracy. A similar observation can be made for sub-models generated using the Mix’n’Match strategy; these models also maintain competitive accuracy.\n",
        "\n",
        "One interesting area for further exploration is the possibility of identifying a sub-model that achieves higher accuracy than the larger models by experimenting with different granularities (number of neurons).\n",
        "\n",
        "In comparison, the traditional transformer model exhibits slightly higher accuracy and was easier to train. However, the difference in accuracy between the traditional transformer and the sub-model $ M_1 $ is not substantial. The MatFormer model stands out because it offers flexibility in model size, allowing for different configurations without the need for retraining from scratch. Retraining models with varying sizes would be computationally expensive, highlighting the efficiency advantage of the MatFormer approach.\n",
        "\n",
        "Overall, the MatFormer model provides a valuable balance between computational efficiency and model performance, making it a versatile choice for applications requiring different model sizes.\n",
        "Note: to train we used only 10 epochs, but probably with more epochs the model will have a better performance\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
